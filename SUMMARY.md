# Summary 

## Development 
I started the development of my project by putting the project prompt into google Gemini to give me project ideas, once I had figured out the idea for my project, I asked Gemini to write a prompt I could give to another LLM in order to make the foundation of my project. This prompt is what I gave to copilot to make the first iteration of my project which just connected to Neo4j and created a blank task. From then on, I used copilot in both agent and edit modes to create the second iteration of my project which I added functions to add and remove tasks from the database. Now that I had a better understanding of both the project and how to use copilot effectively, I started on my final iteration of the project using UV as the basis for the structure of the project and Typer to help make running it easier. For this iteration I gave copilot simple prompt instructions in a markdown document to make sure the code was easy to read so I could debug it easier. Now that I had the foundation of the project figured out I used soley the Agent mode on copilot to implement everything which included add and remove functions from the previous versions, using the chat completions API to suggest tags for tasks, allow sorting by tags, add descriptions to tasks, being able to edit tasks, and linking tasks, all of which it implemented smoothly with only a few hiccups here and there when adjusting the features to make them easier to use. Overall, the AI did a pretty good job given my instructions and it made a functional project. 

## the AI process
Overall, the AI did a good job implementing what I was asking of it without any major issues, I found it struggled a little bit with the creation of .env files and writing how to retrieve information from them, but this was never major and could be easily fixed. I always tested my project by hand without using tools like pytest or having the AI create them, I would run test cases every time I added a new function, and retest old functions to make sure everything worked properly. In the cases where the new function didn't work as intended or a new feature messed up an old function, I would simply describe the issue to the AI and have it fix it, it never took more than two prompts to fix any problem I had on this project. I would I have to say that most of what I tried to due with the AI worked whether it was a simple change, an entire new feature, overhauling a feature, or just asking questions of what to add or improve it rarley failed or misunderstood what I was asking of it. The only places it went wrong was as mentioned it seemed to struggle with .env, also since I did not give it tests to run every so often it would break older functions without knowing that I would have to find by runnin gmy own tests, this would be an improvement I would make on future projects, Also in the last version of my project copilot did struggle with making the chat completions api work, to the point that my own testing couldnt find why it was broke, I asked it to rewrite the function and that fixed it, but of all the functions that kept getting broken throughout the project that one was the most common. But overall the AI did a good job. 



